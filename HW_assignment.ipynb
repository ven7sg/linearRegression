{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e0c7b14a-e5aa-4abc-b48b-b8a9d20dacac",
      "metadata": {
        "id": "e0c7b14a-e5aa-4abc-b48b-b8a9d20dacac"
      },
      "source": [
        "# Assignment: Linear Models\n",
        "## Foundations of Machine Learning\n",
        "## Do Q1 and one other question"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25bf83c6-ff44-42d6-9b33-8be1b945860d",
      "metadata": {
        "id": "25bf83c6-ff44-42d6-9b33-8be1b945860d"
      },
      "source": [
        "**Q1.** Load `./data/Q1_clean.csv`. The data include\n",
        "\n",
        "- `Price` per night\n",
        "- `Review Scores Rating`: The average rating for the property\n",
        "- `Neighbourhood `: The bourough of NYC. Note the space, or rename the variable.\n",
        "- `Property Type`: The kind of dwelling\n",
        "- `Room Type`: The kind of space being rented\n",
        "\n",
        "1. Compute the average prices and scores by `Neighbourhood `; which bourough is the most expensive on average? Create a kernel density plot of price and log price, grouping by `Neighbourhood `.\n",
        "\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.read_csv('/Users/tamerafang/PycharmProjects/DS3002_LinearRegression/Q1_clean.csv')\n",
        "df.head()\n",
        "df.loc[:,['Price','Neighbourhood '] ].groupby('Neighbourhood ').describe()\n",
        "sns.kdeplot(x=df['Price'], hue=df['Neighbourhood '])\n",
        "plt.show()\n",
        "sns.kdeplot(x=np.log(df['Price']), hue=df['Neighbourhood '])\n",
        "plt.show()\n",
        "```\n",
        "* Manhattan is the most expensive.\n",
        "\n",
        "\n",
        "2. Regress price on `Neighbourhood ` by creating the appropriate dummy/one-hot-encoded variables, without an intercept in the linear model and using all the data. Compare the coefficients in the regression to the table from part 1. What pattern do you see? What are the coefficients in a regression of a continuous variable on one categorical variable?\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn import linear_model\n",
        "y = df['Price']\n",
        "x = pd.get_dummies(df['Neighbourhood '], dtype='int')\n",
        "reg = linear_model.LinearRegression(fit_intercept=False).fit(x, y)\n",
        "results = pd.DataFrame({'variable':reg.feature_names_in_, 'coefficient': reg.coef_})\n",
        "print(results)\n",
        "```\n",
        "* The coefficients from part 1 are the same now, showing a pattern that the regression coefficients effectively represent the average prices for each neighbourhood. For a continuous variable on one categorical variable, the coefficients for each category of the categorical variable directly reflect the mean value of the continuous variable for that category.\n",
        "\n",
        "\n",
        "\n",
        "3. Repeat part 2, but leave an intercept in the linear model. How do you have to handle the creation of the dummies differently? What is the intercept? Interpret the coefficients. How can I get the coefficients in part 2 from these new coefficients?\n",
        "```\n",
        "from sklearn import linear_model\n",
        "y = df['Price']\n",
        "x = pd.get_dummies(df['Neighbourhood '], dtype='int', drop_first = True)\n",
        "reg = linear_model.LinearRegression().fit(x, y)\n",
        "results = pd.DataFrame({'variable':reg.feature_names_in_, 'coefficient': reg.coef_})\n",
        "print(results)\n",
        "print(reg.intercept_)\n",
        "results = pd.DataFrame({'variable':reg.feature_names_in_, 'coefficient': reg.coef_+reg.intercept_})\n",
        "print(results)\n",
        "```\n",
        "* When including an intercept in the linear model, you must drop one of the dummy variables created from the 'Neighbourhood' category to avoid multicollinearity and establish a baseline comparison. In this case, the Bronx was dropped. The Brox becomes the reference category and its coefficient from the previous regression becomes the intercept. This makes all the coefficients for this regression are now relative to the Bronx. To get the coefficients from part 2 from these new coefficients, we can add the regression coefficient values to the intercept.\n",
        "\n",
        "\n",
        "4. Split the sample 80/20 into a training and a test set. Run a regression of `Price` on `Review Scores Rating` and `Neighbourhood `. What is the $R^2$ and RMSE on the test set? What is the coefficient on `Review Scores Rating`? What is the most expensive kind of property you can rent?\n",
        "```\n",
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "y = df['Price']\n",
        "x = df.loc[:, ['Review Scores Rating', 'Neighbourhood ']]\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=100)\n",
        "z_train = pd.concat([x_train['Review Scores Rating'], pd.get_dummies(x_train['Neighbourhood '], dtype='int')], axis = 1)\n",
        "z_test = pd.concat([x_test['Review Scores Rating'], pd.get_dummies(x_test['Neighbourhood '], dtype='int')], axis = 1)\n",
        "reg = linear_model.LinearRegression(fit_intercept=False).fit(z_train, y_train)\n",
        "y_hat = reg.predict(z_test)\n",
        "print('Rsq: ', reg.score(z_test, y_test))\n",
        "rmse = np.sqrt( np.mean( (y_test - y_hat)**2 ))\n",
        "print('RMSE: ', rmse)\n",
        "results = pd.DataFrame({'variable':reg.feature_names_in_, 'coefficient': reg.coef_})\n",
        "print(results)\n",
        "answer = 100*1.032257 + 89.4\n",
        "print(answer)\n",
        "```\n",
        "* The Rsq is 0.06701086106947296 and the RMSE is 125.01092061382933. The coefficient on Review Scores Rating is 1.032257. To find the most expensive kind of property you can rent, you would take the calculation of 100*1.032257 + 89.4 to get the answer 192.6257.\n",
        "\n",
        "5. Split the sample 80/20 into a training and a test set. Run a regression of `Price` on `Review Scores Rating` and `Neighbourhood ` and `Property Type`. What is the $R^2$ and RMSE on the test set? What is the coefficient on `Review Scores Rating`? What is the most expensive kind of property you can rent?\n",
        "```\n",
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "y = df['Price']\n",
        "x = df.loc[:, ['Review Scores Rating', 'Neighbourhood ', 'Room Type']]\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=100)\n",
        "z_train = pd.concat([x_train['Review Scores Rating'], pd.get_dummies(x_train['Neighbourhood '], dtype='int'), pd.get_dummies(x_train['Room Type'], dtype='int')], axis = 1)\n",
        "z_test = pd.concat([x_test['Review Scores Rating'], pd.get_dummies(x_test['Neighbourhood '], dtype='int'), pd.get_dummies(x_test['Room Type'], dtype='int')], axis = 1)\n",
        "reg = linear_model.LinearRegression(fit_intercept=False).fit(z_train, y_train)\n",
        "y_hat = reg.predict(z_test)\n",
        "print('Rsq: ', reg.score(z_test, y_test))\n",
        "rmse = np.sqrt( np.mean( (y_test - y_hat)**2 ))\n",
        "print('RMSE: ', rmse)\n",
        "results = pd.DataFrame({'variable':reg.feature_names_in_, 'coefficient': reg.coef_})\n",
        "print(results)\n",
        "answer = 110.617+53.69+100*.0626\n",
        "print(answer)\n",
        "```\n",
        "* The Rsq is 0.2203534812928234 and the RMSE is 114.27692123130632. The coefficient on Review Scores Rating is 0.626912. To find the most expensive kind of property you can rent, you would take the calculation of 110.617+53.69+100*.0626 to get the answer 170.567.\n",
        "\n",
        "\n",
        "6. What does the coefficient on `Review Scores Rating` mean if it changes from part 4 to 5? Hint: Think about how multilple linear regression works.\n",
        "* The coefficient of Review Scores Rating changes in part 4 and 5, as it goes from 1.032257 in part 4 and then to 0.626912 in part 5. This change means there's been an inclusion of additional variables. We added Room Type which can contribute to the variability in room prices. Once we are able to control for room type, the other variables become less powerful predictors so the coefficient decreases.\n",
        "\n",
        "\n",
        "7. (Optional) We've included `Neighborhood ` and `Property Type` separately in the model. How do you interact them, so you can have \"A bedroom in Queens\" or \"A townhouse in Manhattan\". Split the sample 80/20 into a training and a test set and run a regression including that kind of \"property type X neighborhood\" dummy, plus `Review Scores Rating`. How does the slope coefficient for `Review Scores Rating`, the $R^2$, and the RMSE change? Do they increase significantly compares to part 5? Are the coefficients in this regression just the sum of the coefficients for `Neighbourhood ` and `Property Type` from 5? What is the most expensive kind of property you can rent?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95f22300-0180-4ed2-be8f-ed56cf4cd36b",
      "metadata": {
        "id": "95f22300-0180-4ed2-be8f-ed56cf4cd36b"
      },
      "source": [
        "**Q2.** This question is a case study for linear models. The data are about car prices. In particular, they include:\n",
        "\n",
        "  - `Price`, `Color`, `Seating_Capacity`\n",
        "  - `Body_Type`: crossover, hatchback, muv, sedan, suv\n",
        "  - `Make`, `Make_Year`: The brand of car and year produced\n",
        "  - `Mileage_Run`: The number of miles on the odometer\n",
        "  - `Fuel_Type`: Diesel or gasoline/petrol\n",
        "  - `Transmission`, `Transmission_Type`:  speeds and automatic/manual\n",
        "\n",
        "  1. Load `cars_hw.csv`. These data were really dirty, and I've already cleaned them a significant amount in terms of missing values and other issues, but some issues remain (e.g. outliers, badly scaled variables that require a log or arcsinh transformation). Clean the data however you think is most appropriate.\n",
        "\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.read_csv('/Users/tamerafang/PycharmProjects/DS3002_LinearRegression/cars_hw.csv')\n",
        "df0 = df\n",
        "sns.boxplot(data=df)\n",
        "plt.show()\n",
        "print(df.columns)\n",
        "df['price_ihs'] = np.arcsinh(df['Price'])\n",
        "df['mileage_ihs'] = np.arcsinh(df['Mileage_Run'])\n",
        "df['age'] = max(df['Make_Year'])-df['Make_Year']\n",
        "df = df.drop(['Price','Mileage_Run','Make_Year','Unnamed: 0'],axis=1)\n",
        "df.boxplot()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n",
        "  2. Summarize the `Price` variable and create a kernel density plot. Use `.groupby()` and `.describe()` to summarize prices by brand (`Make`). Make a grouped kernel density plot by `Make`. Which car brands are the most expensive? What do prices look like in general?\n",
        "\n",
        "\n",
        "```\n",
        "price_summary = df0['Price'].describe()\n",
        "print(price_summary)\n",
        "price_kd = sns.kdeplot(data = df0, x='Price',hue='Make')\n",
        "df0['Price'].groupby(df0['Make']).describe()\n",
        "plt.show()\n",
        "price_ihs_kd = sns.kdeplot(data=df,x='price_ihs',hue='Make')\n",
        "df['price_ihs'].groupby(df['Make']).describe()\n",
        "plt.show()\n",
        "```\n",
        "* MG Motor cars are most expensive, followed by Kia and Jeep. In general, prices range from 188,000 to 2,941,000 rupees. The average price is 741,019.5 rupees.\n",
        "\n",
        "\n",
        "  3. Split the data into an 80% training set and a 20% testing set.\n",
        "  ```\n",
        "N = df.shape[0]\n",
        "df = df.sample(frac=1, random_state=100)\n",
        "train_size = int(.8*N)\n",
        "df_train = df[0:train_size]\n",
        "y_train = df_train['price_ihs']\n",
        "df_test = df[train_size:]\n",
        "y_test = df_test['price_ihs']\n",
        "```\n",
        "\n",
        "  4. Make a model where you regress price on the numeric variables alone; what is the $R^2$ and `RMSE` on the training set and test set? Make a second model where, for the categorical variables, you regress price on a model comprised of one-hot encoded regressors/features alone (you can use `pd.get_dummies()`; be careful of the dummy variable trap); what is the $R^2$ and `RMSE` on the test set? Which model performs better on the test set? Make a third model that combines all the regressors from the previous two; what is the $R^2$ and `RMSE` on the test set? Does the joint model perform better or worse, and by home much?\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Model 1: Numeric Variables Only\n",
        "numeric_vars = ['age', 'mileage_ihs', 'Seating_Capacity']\n",
        "X_train_numeric = df_train[numeric_vars]\n",
        "X_test_numeric = df_test[numeric_vars]\n",
        "regressor_numeric = LinearRegression().fit(X_train_numeric, y_train)\n",
        "y_pred_numeric = regressor_numeric.predict(X_test_numeric)\n",
        "print('Numeric only R-squared: ', regressor_numeric.score(X_test_numeric, y_test))\n",
        "rmse_numeric = np.sqrt(mean_squared_error(y_test, y_pred_numeric))\n",
        "print('Numeric only RMSE: ', rmse_numeric)\n",
        "\n",
        "# Model 2: Categorical Variables Only\n",
        "categorical_vars = ['Make', 'Body_Type', 'Color', 'Fuel_Type', 'Transmission', 'Transmission_Type']\n",
        "dummies_train = pd.DataFrame()\n",
        "dummies_test = pd.DataFrame()\n",
        "for var in categorical_vars:\n",
        "    dummies = pd.get_dummies(df[var], drop_first=True, dtype=int)\n",
        "    dummies_train = pd.concat([dummies_train, dummies.iloc[:train_size]], axis=1)\n",
        "    dummies_test = pd.concat([dummies_test, dummies.iloc[train_size:]], axis=1)\n",
        "regressor_categorical = LinearRegression().fit(dummies_train, y_train)\n",
        "y_pred_categorical = regressor_categorical.predict(dummies_test)\n",
        "print('Categorical only R-squared: ', regressor_categorical.score(dummies_test, y_test))\n",
        "rmse_categorical = np.sqrt(mean_squared_error(y_test, y_pred_categorical))\n",
        "print('Categorical only RMSE: ', rmse_categorical)\n",
        "\n",
        "# Model 3: Combined Numeric and Categorical Variables\n",
        "X_train_combined = pd.concat([X_train_numeric, dummies_train], axis=1)\n",
        "X_test_combined = pd.concat([X_test_numeric, dummies_test], axis=1)\n",
        "regressor_combined = LinearRegression().fit(X_train_combined, y_train)\n",
        "y_pred_combined = regressor_combined.predict(X_test_combined)\n",
        "print('Combined R-squared: ', regressor_combined.score(X_test_combined, y_test))\n",
        "rmse_combined = np.sqrt(mean_squared_error(y_test, y_pred_combined))\n",
        "print('Combined RMSE: ', rmse_combined)\n",
        "```\n",
        "* Numeric only R-squared:  0.45254262356326824\n",
        "Numeric only RMSE:  0.33392654735906463\n",
        "Categorical only R-squared:  0.6298129532407464\n",
        "Categorical only RMSE:  0.27459106425227275\n",
        "Combined R-squared:  0.7999206763763922\n",
        "Combined RMSE:  0.20187237686198908\n",
        "* The joint model performs the best, for we see a significant improvement in R-squared and a reduction in RMSE.\n",
        "\n",
        "\n",
        "  5. Use the `PolynomialFeatures` function from `sklearn` to expand the set of numerical variables you're using in the regression. As you increase the degree of the expansion, how do the $R^2$ and `RMSE` change? At what point does $R^2$ go negative on the test set? For your best model with expanded features, what is the $R^2$ and `RMSE`? How does it compare to your best model from part 4?\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "numeric_vars = ['age', 'mileage_ihs', 'Seating_Capacity']\n",
        "categorical_vars = ['Make', 'Body_Type', 'Color', 'Fuel_Type', 'Transmission', 'Transmission_Type']\n",
        "\n",
        "dummies = pd.get_dummies(df[categorical_vars], drop_first=True)\n",
        "dummies_train = dummies.iloc[:train_size]\n",
        "dummies_test = dummies.iloc[train_size:]\n",
        "\n",
        "for degree in np.arange(1, 5):\n",
        "    polynomial_expander = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    X_train_numeric_expanded = polynomial_expander.fit_transform(df_train[numeric_vars])\n",
        "    X_test_numeric_expanded = polynomial_expander.transform(df_test[numeric_vars])\n",
        "    expanded_feature_names = polynomial_expander.get_feature_names_out(numeric_vars)\n",
        "    X_train_numeric_expanded_df = pd.DataFrame(X_train_numeric_expanded, columns=expanded_feature_names)\n",
        "    X_test_numeric_expanded_df = pd.DataFrame(X_test_numeric_expanded, columns=expanded_feature_names)\n",
        "    X_train_combined = pd.concat([X_train_numeric_expanded_df, dummies_train.reset_index(drop=True)], axis=1)\n",
        "    X_test_combined = pd.concat([X_test_numeric_expanded_df, dummies_test.reset_index(drop=True)], axis=1)\n",
        "    regressor_combined_expanded = LinearRegression().fit(X_train_combined, y_train)\n",
        "    y_pred_combined_expanded = regressor_combined_expanded.predict(X_test_combined)\n",
        "    r_squared_expanded = regressor_combined_expanded.score(X_test_combined, y_test)\n",
        "    rmse_expanded = np.sqrt(mean_squared_error(y_test, y_pred_combined_expanded))\n",
        "    print(f'Degree {degree} R-squared: {r_squared_expanded}')\n",
        "    print(f'Degree {degree} RMSE: {rmse_expanded}')\n",
        "```\n",
        "* As degree increases from 1 to 2 and 2 to 3, R-squared and RMSE experience minor increases and decreases but generally stay around the same value. R-squared becomes negative at degree 4. The best degrees is 2 with a R-squared of 0.8025408094604638 and RMSE of 0.20054621389085775. This is just slightly better than our best model from part 4.\n",
        "\n",
        "  6. For your best model so far, determine the predicted values for the test data and plot them against the true values. Do the predicted values and true values roughly line up along the diagonal, or not? Compute the residuals/errors for the test data and create a kernel density plot. Do the residuals look roughly bell-shaped around zero? Evaluate the strengths and weaknesses of your model.\n",
        "\n",
        "\n",
        "```\n",
        "best_degree = 2\n",
        "\n",
        "polynomial_expander = PolynomialFeatures(degree=best_degree, include_bias=False)\n",
        "X_train_numeric_expanded = polynomial_expander.fit_transform(df_train[numeric_vars])\n",
        "X_test_numeric_expanded = polynomial_expander.transform(df_test[numeric_vars])\n",
        "expanded_feature_names = polynomial_expander.get_feature_names_out(numeric_vars)\n",
        "X_train_numeric_expanded_df = pd.DataFrame(X_train_numeric_expanded, columns=expanded_feature_names)\n",
        "X_test_numeric_expanded_df = pd.DataFrame(X_test_numeric_expanded, columns=expanded_feature_names)\n",
        "X_train_combined = pd.concat([X_train_numeric_expanded_df, dummies_train.reset_index(drop=True)], axis=1)\n",
        "X_test_combined = pd.concat([X_test_numeric_expanded_df, dummies_test.reset_index(drop=True)], axis=1)\n",
        "regressor_combined_expanded = LinearRegression().fit(X_train_combined, y_train)\n",
        "y_pred_combined_expanded = regressor_combined_expanded.predict(X_test_combined)\n",
        "\n",
        "residuals = y_test - y_pred_combined_expanded\n",
        "\n",
        "# Scatter plot of true vs. predicted values\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=y_test, y=y_pred_combined_expanded)\n",
        "plt.xlabel('True Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('True Values vs. Predicted Values')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)  # Diagonal line for reference\n",
        "plt.show()\n",
        "\n",
        "# Kernel density plot of residuals\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.kdeplot(residuals, shade=True)\n",
        "plt.title('Kernel Density Plot of Residuals')\n",
        "plt.xlabel('Residuals')\n",
        "plt.ylabel('Density')\n",
        "plt.show()\n",
        "\n",
        "from sklearn import tree\n",
        "\n",
        "X_train_numeric = df_train[numeric_vars].reset_index(drop=True)\n",
        "X_train_categorical = dummies_train.reset_index(drop=True)\n",
        "X_train_all = pd.concat([X_train_numeric, X_train_categorical], axis=1)\n",
        "\n",
        "X_test_numeric = df_test[numeric_vars].reset_index(drop=True)\n",
        "X_test_categorical = dummies_test.reset_index(drop=True)\n",
        "X_test_all = pd.concat([X_test_numeric, X_test_categorical], axis=1)\n",
        "\n",
        "best_depth = 0\n",
        "best_rmse = float('inf')\n",
        "best_r2 = float('-inf')\n",
        "\n",
        "sup_depth = 20\n",
        "for depth in np.arange(2, sup_depth):\n",
        "    decision_tree_model = tree.DecisionTreeRegressor(max_depth=depth)\n",
        "    decision_tree_model.fit(X_train_all, y_train)\n",
        "    y_pred_tree = decision_tree_model.predict(X_test_all)\n",
        "    rmse_tree = np.sqrt(mean_squared_error(y_test, y_pred_tree))\n",
        "    r2_tree = decision_tree_model.score(X_test_all, y_test)\n",
        "    print(f'Depth: {depth}, RMSE: {rmse_tree:.4f}, R-squared: {r2_tree:.4f}')\n",
        "\n",
        "```\n",
        "* The model is generally a good fit because predicted values align with true values across an upward trend, though the alignment deviates at higher values. We also see that the residuals center around zero in a bell-shaped distribution. However, there is somw right skewness and increased spread at higher values which may mean the model's tendency to underestimate larger prices.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bd15c6b-4c7c-4230-a199-e03e1054ec6a",
      "metadata": {
        "id": "7bd15c6b-4c7c-4230-a199-e03e1054ec6a"
      },
      "source": [
        "**Q3.** This is a question about linear regression. The outcome is whether a defendant is held pre-trial in the Virginia justice system. We would like to understand how that outcome is predicted by characteristics of the defendant, particularly race. Let's be very careful/clear: We aren't saying anyone *should* be held without bond or asserting that people with different demographic variables *should* be more likely to be held, but instead trying to predict whether people with different characteristics *are empirically more likely* to be held without bond, given the available information. This is the first step we would take in investigating whether a system is fair, or how large the disparities are: Does it treat people with similar observable characteristics similarly, or not? We are going to look at a common question: Are Black defendants treated differently from white or Asian ones? (There are Native American defendants, but there are 11 in total, which is such a small number of observations that is difficult to clearly say anything about how this group is treated relative to the others.)\n",
        "\n",
        "The variables in the data are:\n",
        "\n",
        "  - `held_wo_bail`: Whether a defendant is held without bail before trial (Boolean logical)\n",
        "  - `race`, `sex`: Categorical demographic variables\n",
        "  - `is_poor`: Whether the defendant is classified as indigent\n",
        "  - `prior_F`, `prior_M`: The number of prior felony and misdemeanor arrests\n",
        "  - `case_type`: A categorical variable indicating a misdemeanor `M` or felony `F` or infraction `I` or special case `S`\n",
        "  - `age`: Defendant's age\n",
        "  - `bond`, `bond_NA`, `bond_type`: The amount of any bond, whether it is missing, and the type\n",
        "  - `sentence`, `sentence_NA`, `sentence_type`: The length of any sentence, whether it is missing, and the type\n",
        "\n",
        "1. Load the `pretrial_data.csv` data. Notice that there are `nan`s, but the data are relatively clean. Because there are `.nan`s among variables you won't use, you'll want to narrow down your analysis to the relevant variables before dropping or imputing missing values.\n",
        "2. Create a dummy variable indicating that the defendant is Black.\n",
        "3. Regress `held` on `Black`. What is the slope coefficient Interpret the coefficient on the Black dummy variable: How much more likely is a black person to be held without bail? What is the $R^2$ of the model?\n",
        "4. Before doing this question, please think for a few minutes about how to make the process of running the following regressions as efficient as possible, before jumping into writing code. Repeat part 2, for the following specifications, keeping track of the coefficient on the Black dummy variable each time:\n",
        "      - `held` on `Black` and `sex`\n",
        "      - `held` on `Black` and `sex` and `is_poor`\n",
        "      - `held` on `Black` and `sex` and `is_poor` and `prior_F`\n",
        "      - `held` on `Black` and `sex` and `is_poor` and `prior_F` and `case_type`\n",
        "What happens to the coefficient on the Black dummy variable as you include more regressors/features/controls in the regression? Explain your findings.\n",
        "5. Suppose we don't want to see just `Black` and `sex`, but `Black` interacted with `sex`: Are Black men and Black women treated systemically differently from the rest of the population? Implement this in a regression, and explain your findings.\n",
        "6. Imagine someone argued we should use these kinds of models to help a judge or magistrate make bail decisions (you could obviously go back and make this kind of model for the bond and sentence variables, then deploy it on new cases to predict what their bond and sentence values would be). What concerns would you have? Do you think society should be using data-driven and automated tools like that? Explain your concerns clearly."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0bedb79-b3d9-4db3-9b30-b92c9b618cec",
      "metadata": {
        "id": "d0bedb79-b3d9-4db3-9b30-b92c9b618cec"
      },
      "source": [
        "**Q4.** This is a math question to review the derivation of the OLS estimator (but only if you are into that kind of thing!). We are going to do it slightly differently from what we did in class, though. We will use a linear predictor and minimize the Sum of Squared Errors, just as in class. But, we are going to de-mean $X$ first, creating another variable $z_i = x_i - \\bar{x}$ where\n",
        "$$\n",
        "\\bar{x} = \\dfrac{1}{N} \\sum_{i=1}^N x_i,\n",
        "$$\n",
        "so the model is $\\hat{y}_i = a + b z_i$ and the `SSE` is\n",
        "$$\n",
        "\\text{SSE}(a,b) = \\sum_{i=1}^N (y_i - a - bz_i)^2.\n",
        "$$\n",
        "\n",
        "  1. Take partial derivatives of the `SSE` with respect to $a$ and $b$. You should get\n",
        "\n",
        "\\begin{alignat*}{3}\n",
        "\\sum_{i=1}^N -2(y_i - a- bz_i) &=& 0 \\\\\n",
        "\\sum_{i=1}^N -2(y_i - a - bz_i)z_i &=& 0.\n",
        "\\end{alignat*}\n",
        "\n",
        "  2. Solve for the solutions to the above equations. Big hint: $\\bar{z} = 0$, since we subtracted the mean of $x$ from $x$ to get $z$. You should get\n",
        "\n",
        "\\begin{alignat*}{3}\n",
        "a^* &=& \\bar{y} \\\\\n",
        "b^* &=& \\dfrac{\\sum_{i=1}^N(y_i - \\bar{y})z_i}{\\sum_{i=1}^N z_i^2}.\n",
        "\\end{alignat*}\n",
        "\n",
        "  3. Substitute $z_i = x_i - \\bar{x}$ back into the above equations. You should get\n",
        "  \n",
        "\\begin{alignat*}{3}\n",
        "a^* &=& \\bar{y} \\\\\n",
        "b^* &=& \\dfrac{\\sum_{i=1}^N(y_i - \\bar{y})(x_i-\\bar{x})}{\\sum_{i=1}^N (x_i-\\bar{x})^2},\n",
        "\\end{alignat*}\n",
        "\n",
        "which can be written in terms of sample covariance and sample variance as:\n",
        "\n",
        "\\begin{alignat*}{3}\n",
        "a^* &=& \\bar{y} \\\\\n",
        "b^* &=& \\dfrac{\\text{cov}(x,y)}{\\text{var}(x)}.\n",
        "\\end{alignat*}\n",
        "\n",
        "This is typically the preferred way of expressing the OLS coefficients.\n",
        "\n",
        "4. When will $b^*$ be large or small, depending on the relationship between $x$ and $y$ and the amount of \"noise\"/variance in $x$? What does $a^*$ represent?\n",
        "5. Suppose you have measurement error in $x$ which artificially inflates its variance (e.g. bad data cleaning). What happens to the $b^*$ coefficient? How will affect your ability to predict? (This phenomenon is called **attenuation**.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b67478ac-ad78-4a44-9720-583c71b8da14",
      "metadata": {
        "id": "b67478ac-ad78-4a44-9720-583c71b8da14"
      },
      "source": [
        "**Q5.**\n",
        "1. Find a dataset on a topic you're interested in. Some easy options are data.gov, kaggle.com, and data.world.\n",
        "2. Clean the data and do some exploratory data analysis on key variables that interest you. Pick a particular target/outcome variable and features/predictors.\n",
        "3. Split the sample into an ~80% training set and a ~20% test set.\n",
        "4. Run a few regressions of your target/outcome variable on a variety of features/predictors. Compute the SSE on the test set.\n",
        "5. Which model performed the best, and why?\n",
        "6. What did you learn?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}